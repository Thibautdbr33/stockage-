# TP2 : Modest Storage SAN Compte Rendu
# Partie 0 : PrÃ©requis 
- Avoir GNS3 
- Avoir VBox 
- Les isos nÃ©cessaires (switch, serveurs)

# Partie I : Setup GNS

## 1. Tableau d'adressage

| Node            | `mgmt-lan`     | `sto1-lan`   | `sto2-lan`   |
|-----------------|----------------|--------------|--------------|
| `sto1.tp2.b3`   | x              | `10.3.1.1`   | `10.3.2.1`   |
| `sto2.tp2.b3`   | x              | `10.3.1.2`   | `10.3.2.2`   |
| `chunk1.tp2.b3` | `10.3.250.11`  | `10.3.1.101` | `10.3.2.101` |
| `chunk2.tp2.b3` | `10.3.250.12`  | `10.3.1.102` | `10.3.2.102` |
| `chunk3.tp2.b3` | `10.3.250.13`  | `10.3.1.103` | `10.3.2.103` |
| `master.tp2.b3` | `10.3.250.1`   | x            | x            |
| `web.tp2.b3`    | `10.3.250.101` | x            | x            |

## 2. Setup rÃ©seau et GNS3

âœ **Clonez votre machine Linux autant de fois que nÃ©cessaire**
  - CrÃ©er une machine linux (j'ai choisi Debian 12 dÃ©solÃ©) avec un utilisteur qui Ã  les droits suoders, le service ssh installÃ© et activÃ©, firewall installÃ© et activÃ© avec le ssh allowed et une carte NAT pour avoir internet sur toutes les machines 
  - clonÃ© la machine 6 fois pour avoir 7 machines (master, web, chunk1, chunk2, chunk3, sto1, sto2)
  - Avoir la VM GNS3 dans VBox 
  - Importer l'image du switch et lui mettre sa licence

- **Configurer les interfaces rÃ©seau**
  - Importer les VM dans GNS3
  - Ajouter les cartes rÃ©seaux aux machines sur GNS3 et les connecter comme demandÃ© dans le tableau d'adressage
  - Configurer les interfaces rÃ©seau sur chaque machine avec les adresses en fonction du tableau d'adressage 

- **ajoutez une carte host-only supplÃ©mentaire Ã  la machine `master.tp2.b3`**
  - Pour l'utiliserez pour se connecter en SSH
  - on indique encore une carte de + dans GNS3 pour `master.tp2.b3`
  - ce sera notre bastion SSH : on se connectera Ã  lui pour se connecter aux autres machines

âœ **Avoir une connexion SSH sur toutes les machines du RÃ©seau**

  - Sur la machine physique, gÃ©nÃ©rer une parire de clÃ© ssh `ssh-keygen -t rsa -b 4096 -C` 

  - CrÃ©er un fichier config lÃ  oÃ¹ est la clÃ© ssh gÃ©nÃ©rÃ©e `C:\Users\thiba\.ssh` pour utiliser la machine `master.tp2.b3` comme bastion SSH grÃ¢ce au Proxyjump (Utiliser master pour accÃ©der aux autres machines du rÃ©seau et chunk1 pour accÃ©der Ã  sto1 et sto2) :
  ```
  Host master
    HostName 192.168.148.5
    User user1

Host chunk1
    HostName 10.3.250.11
    User user1
    ProxyJump master

Host chunk2
    HostName 10.3.250.12
    User user1
    ProxyJump master

Host chunk3
    HostName 10.3.250.13
    User user1
    ProxyJump master

Host web
    HostName 10.3.250.101
    User user1
    ProxyJump master

Host sto1
    HostName 10.3.1.1
    User user1
    ProxyJump chunk1

Host sto2
    HostName 10.3.1.2
    User user1
    ProxyJump chunk1
  ```

 - On copie la clÃ© publique gÃ©nÃ©rÃ©e `C:\Users\thiba\.ssh\id_rsa.pub` dans le fichier `authorized_keys` de toutes les machines du rÃ©seau pour ne pas Ã  avoir mettre le mot de passe Ã  chaque fois qu'on se connecte.

âœ **Attribuez un hostname Ã  chaque machine**

- avec une commande `hostnamectl set-hostname`

âœ **Remplir le fichier `hosts` de chaque machine**
  ```
  ```
- comme Ã§a on utilisera parfois les noms des machines pour mettre en place la config
- On vÃ©rifie avec quelques pings que Ã§a peut se joindre avec les noms

# II. SAN network

Dans cette partie, on va travailler autour du (modeste) rÃ©seau SAN : les machines de stockage (`sto1` et `sto2`) ainsi que les machines qui vont consommer ce stockage : les "chunks" (`chunk1`, `chunk2` et `chunk3`).

L'objectif est le suivant :

- **les machines de stockage**
  - ont plein plein de disques
  - ont du RAID configurÃ©
  - propose les volumes RAID Ã  travers le rÃ©seau comme *target iSCSI*
- **les machines "chunks"**
  - accÃ¨dent aux disque des machines de stockage
  - grÃ¢ce Ã  iSCSI
  - mise en place d'un *multipathing iSCSI*

Le *multipathing* permet de prÃ©venir d'une Ã©ventuelle dÃ©faillance d'un lien rÃ©seau en proposant deux chemins d'accÃ¨s rÃ©seau pour accÃ©der Ã  un disque donnÃ© en iSCSI.

> *En milieu pro, dans des environnements de grande taille, on voit aussi beaucoup de cÃ¢bles fibre optique, et le protocole FC (Fiber Channel) qui remplace iSCSI. Il faut alors utiliser des switches spÃ©cialisÃ©s (la gamme 9000 chez Cisco : les switches MDS, spÃ©cifiques pour les SAN). Les concepts restent les mÃªmes : FC transporte du trafic SCSI.*

Pour setup tout Ã§a, on va utiliser les tools standards de Open iSCSI.


## 1. Storage machines

> **Toutes les manips de cettes section sont Ã  rÃ©aliser sur `sto1` et `sto2`.**

### A. Disks and RAID

On va au supermarchÃ© (celui de VBox, c'est gratuit) et on ajoute plein de disques aux machines, pour configurer du RAID, notre premier niveau de redondance.

âœ **Ajouter des disques aux machines `sto1` et `sto2`**

- combien ? de quelle taille ? en vrai faites vous plais
- Ã  la fin on veut (au minimum) 3 volumes RAID sur chaque `sto`
- au plus simple
  - 6 disques, pour faire 3 RAID1
  - au minimum 1G par disque
- **la mÃªme taille pour tous les disques**

ğŸŒ **Configurer des RAID**

- Ã  la fin, on veut 3 volumes RAID sur `sto1` et 3 volumes RAID sur `sto2`
- vous pouvez faire le setup simple indiquÃ© avant ou vous faire un peu plais et partir sur du RAID5 par exemple
- **il faut 3 volumes RAID prÃªts Ã  l'emploi en fin de setup**
- vous utiliserez MDADM pour mettre en place les RAID

ğŸŒ **Prouvez que vous avez 3 volumes RAID prÃªts Ã  l'emploi**

- avec une commande `lsblk`


### B. iSCSI target

On va configurer nos deux machines `sto1` et `sto2` pour qu'elles exposent leurs volumes RAID sur le rÃ©seau, avec iSCSI.

On appelle *target iSCSI* un device exposÃ© sur le rÃ©seau grÃ¢ce au protocole iSCSI.

On appelle *iSCSI initiator* une machine qui va initier une connexion iSCSI pour accÃ©der Ã  un *target iSCSI*.

Sur les machines `sto1` et `sto2`, on va donc s'occuper pour le moment de les configurer pour qu'elles exposent les volumes RAID comme *target iSCSI*.

Pour Ã§a, on va utiliser l'outil `target` dispo sous les OS Linux.

ğŸŒ **Installer `target`**

- c'est le paquet `targetcli`

ğŸŒ **DÃ©marrer le service `target`**

- activez le aussi au dÃ©marrage de la machine

ğŸŒ **Configurer les *targets iSCSI***

- il faut configurer :
  - 3 target iSCSI sur chaque machine : un pour exposer chacun des volumes RAID
  - une ACL sur chaque target (vous pouvez try de mettre une authent, mais je vous conseille de rester simple)
- chaque target doit respecter la convention de nommage pour son IQN :
  - `iqn.2024-12.tp2.b3:data-chunk1` pour le premier target iSCSI (destinÃ© Ã  Ãªtre utilisÃ© par la machine `chunk1.tp2.b3`)
  - `iqn.2024-12.tp2.b3:data-chunk2` pour le deuxiÃ¨me
  - et `iqn.2024-12.tp2.b3:data-chunk3`
- en utilisant `target-cli`
  - exemple pour crÃ©er un target

```bash
$ sudo targetcli

# on crÃ©e un objet fileio que target peut gÃ©rer Ã  partir de notre volume RAID
/> /backstores/fileio create name=data-chunk1 file_or_dev=/dev/path/vers/RAID

# on crÃ©e un IQN : un identifiant iSCSI unique
/> /iscsi create iqn.2024-12.tp2.b3:data-chunk1

# on crÃ©e une ACL pour que notre initator puisse accÃ©der Ã  ce target iSCSI
/> /iscsi/iqn.2024-12.tp2.b3:data-chunk1/tpg1/acls create iqn.2024-12.tp2.b3:data-chunk1:chunk1-initiator

# on map un LUN de notre target iSCSI vers notre objet fileio (le volume RAID)
/> /iscsi/iqn.2024-12.tp2.b3:data-chunk1/tpg1/luns/ create /backstores/fileio/data-chunk1

/> saveconfig
/> exit
```

âœ **Vous devriez avoir un truc comme Ã§a une fois en place, avec les trois volumes RAID exposÃ©s comme target :**

```bash
[it4@sto1 ~]$ sudo targetcli
targetcli shell version 2.1.57
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/> ls
o- / ......................................................... [...]
  o- backstores .............................................. [...]
  | o- block .................................. [Storage Objects: 0]
  | o- fileio ................................. [Storage Objects: 3]
  | | o- chunk1 ... [/dev/md/chunk1 (511.0MiB) write-back activated]
  | | | o- alua ................................... [ALUA Groups: 1]
  | | |   o- default_tg_pt_gp ....... [ALUA state: Active/optimized]
  | | o- chunk2 ... [/dev/md/chunk2 (511.0MiB) write-back activated]
  | | | o- alua ................................... [ALUA Groups: 1]
  | | |   o- default_tg_pt_gp ....... [ALUA state: Active/optimized]
  | | o- chunk3 ... [/dev/md/chunk3 (511.0MiB) write-back activated]
  | |   o- alua ................................... [ALUA Groups: 1]
  | |     o- default_tg_pt_gp ....... [ALUA state: Active/optimized]
  | o- pscsi .................................. [Storage Objects: 0]
  | o- ramdisk ................................ [Storage Objects: 0]
  o- iscsi ............................................ [Targets: 3]
  | o- iqn.2024-12.tp2.b3:data-chunk1 .................... [TPGs: 1]
  | | o- tpg1 ............................... [no-gen-acls, no-auth]
  | |   o- acls .......................................... [ACLs: 1]
  | |   | o- iqn.2024-12.tp2.b3:chunk1-initiator .. [Mapped LUNs: 1]
  | |   |   o- mapped_lun0 ............... [lun0 fileio/chunk1 (rw)]
  | |   o- luns .......................................... [LUNs: 1]
  | |   | o- lun0 .. [fileio/chunk1 (/dev/md/chunk1) (default_tg_pt_gp)]
  | |   o- portals .................................... [Portals: 1]
  | |     o- 0.0.0.0:3260 ..................................... [OK]
  | o- iqn.2024-12.tp2.b3:data-chunk2 .................... [TPGs: 1]
  | | o- tpg1 ............................... [no-gen-acls, no-auth]
  | |   o- acls .......................................... [ACLs: 1]
  | |   | o- iqn.2024-12.tp2.b3:chunk2-initiator .. [Mapped LUNs: 1]
  | |   |   o- mapped_lun0 ............... [lun0 fileio/chunk2 (rw)]
  | |   o- luns .......................................... [LUNs: 1]
  | |   | o- lun0 .. [fileio/chunk2 (/dev/md/chunk2) (default_tg_pt_gp)]
  | |   o- portals .................................... [Portals: 1]
  | |     o- 0.0.0.0:3260 ..................................... [OK]
  | o- iqn.2024-12.tp2.b3:data-chunk3 .................... [TPGs: 1]
  |   o- tpg1 ............................... [no-gen-acls, no-auth]
  |     o- acls .......................................... [ACLs: 1]
  |     | o- iqn.2024-12.tp2.b3:chunk3-initiator .. [Mapped LUNs: 1]
  |     |   o- mapped_lun0 ............... [lun0 fileio/chunk3 (rw)]
  |     o- luns .......................................... [LUNs: 1]
  |     | o- lun0 .. [fileio/chunk3 (/dev/md/chunk3) (default_tg_pt_gp)]
  |     o- portals .................................... [Portals: 1]
  |       o- 0.0.0.0:3260 ..................................... [OK]
  o- loopback ......................................... [Targets: 0]
```

## 2. Chunks machine

> **On passe sur juste `chunk1.tp2.b3` pour cette partie.** Vous rÃ©pliquerez la conf que vous allez tester sur lui une fois que c'est ok sur les deux autres : `chunk2.tp2.b3` et `chunk3.tp2.b3`. Uniquement sur `chunk1.tp2.b3` pour le moment donc.

Ici, on va configurer `chunk1.tp2.b3` pour qu'il utilise un *iSCSI initiator* afin d'accÃ©der aux volumes RAID proposÃ©s sur le rÃ©seau par `sto1` et `sto2`.

Y'a un tool trÃ¨s rÃ©pandu sur les OS Linux pour utiliser iSCSI : `iscsiadm` et le dÃ©mon `iscsid`.

Avec `iscsiadm` on peut :

- dÃ©couvrir des target iSCSI en indiquant l'IP Ã  laquelle on souhaite se connecter
  - on indiquera les IP de nos machines `sto1` et `sto2`
- configurer un *initiator*
  - pour se connecter et se *login* en iSCSI Ã  nos targets

### A. Simple iSCSI

ğŸŒ **Installer les tools iSCSI sur `chunk1.tp2.b3`**

- c'est le paquet `iscsi-initiator-utils`

ğŸŒ **Configurer un iSCSI initiator**

- utilisez `iscsiadm` pour dÃ©couvrir les *targets iSCSI* proposÃ©s par `sto1` et `sto2`
- faites vos recherches pour la conf, c'est une conf plutÃ´t standard
- les commandes `iscsiadm` gÃ©nÃ¨rent des fichiers dans `/var/lib/iscsi`
- dÃ©couvrez et utilisez les targets destinÃ©s Ã  cette machine, y'en a 4 :
  - `iqn.2024-12.tp2.b3:chunk1` sur
    - `sto1` donc : `10.3.1.1` et `10.3.1.2`
    - `sto2` donc : `10.3.2.1` et `10.3.2.2`
- n'oubliez pas d'activer dÃ¨s le dÃ©marrage de la machine les services iSCSI
  - service `iscsi`
  - service `iscsid`
- je vous donne la premiÃ¨re commande :

```bash
sudo iscsiadm -m discoverydb -t st -p 10.3.1.1:3260 --discover
```

> On peut par exemple aussi lister les hÃ´tes dÃ©couverts et utilisables avec `sudo iscsiadm -m node`.

ğŸŒ **Modifier la configuration du dÃ©mon iSCSI**

- le fichier `/etc/iscsi/iscsid.conf`
- modifier la ligne `node.session.timeo.replacement_timeout`
- mettez lui 0 pour valeur, Ã§a donne : `node.session.timeo.replacement_timeout = 0`
- redÃ©marrer les services `iscsi` et `iscsid`

> *Vivement recommandÃ© d'aller voir ce que fait cette option de config !*

ğŸŒ **Prouvez que la configuration est prÃªte**

- avec `lsblk` on doit voir les volumes
  - on doit donc voir 4 volumes en iSCSI
  - y'en a que deux en rÃ©alitÃ© (un proposÃ© par `sto1` et un proposÃ© par `sto2`) mais on les utilise depuis deux IPs diffÃ©rents Ã  chaque fois, donc on les voit en double
- une commande `iscsiadm -m session -P 3` aussi dans le compte-rendu : on y voit en dÃ©tails les connexions iSCSI en cours

### B. Multipathing

> Toujouuurs sur `chunk1.tp2.b3`.

Le *multipathing* va terminer le setup iSCSI. Actuellement, notre machine `chunk1` voit 4 volumes, alors qu'il en existe que 2 en rÃ©alitÃ©.

On va configurer le *multipathing* pour que la machine `chunk1` ne voit que deux volumes, chacun ayant deux chemins d'accÃ¨s possibles : **on ajoute un niveau de redondance pour tolÃ©rer les pannes rÃ©seau.**

ğŸŒ **Installer les outils multipath sur `chunk1.tp2.b3`**

- c'est le paquet `device-mapper-multipath`

ğŸŒ **Configurer le fichier `/etc/multipath.conf`**

- rÃ©cupÃ©rez le fichier d'exemple `usr/share/doc/device-mapper-multipath/multipath.conf`
- modifier la section ``defaults` comme ceci :

```conf
defaults {
  user_friendly_names yes
  find_multipaths yes
  path_grouping_policy failover
  features "1 queue_if_no_path"
  no_path_retry 100
}
```

ğŸŒ **DÃ©marrer le service `multipathd`**

ğŸŒ **Et euh c'est tout, il est smart enough**

- ptit `lsblk`
  - vous devriez avoir des volumes `mpatha` et `mpathb`
- on peut aussi `multipath -ll` pour avoir + d'infos sur l'Ã©tat du *multipathing*

âœ **On aboutit sur un truc comme Ã§a :**

```bash
[it4@chunk1 ~]$ sudo multipath -ll
mpatha (36001405245a1502fa7244cba904620eb) dm-2 LIO-ORG,chunk2
size=511M features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| `- 4:0:0:0 sdd 8:48 active ready running
`-+- policy='service-time 0' prio=50 status=enabled
  `- 5:0:0:0 sdc 8:32 active ready running
mpathb (3600140502598561316949c98c14c64d7) dm-4 LIO-ORG,chunk2
size=511M features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| `- 6:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=50 status=enabled
  `- 3:0:0:0 sdb 8:16 active ready running

[it4@chunk1 ~]$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS
sda           8:0    0    8G  0 disk  
â”œâ”€sda1        8:1    0    1G  0 part  /boot
â””â”€sda2        8:2    0    7G  0 part  
  â”œâ”€rl-root 253:0    0  6.2G  0 lvm   /
  â””â”€rl-swap 253:1    0  820M  0 lvm   [SWAP]
sdb           8:16   0  511M  0 disk  
â””â”€mpathb    253:4    0  511M  0 mpath 
sdc           8:32   0  511M  0 disk  
â””â”€mpatha    253:2    0  511M  0 mpath 
sdd           8:48   0  511M  0 disk  
â””â”€mpatha    253:2    0  511M  0 mpath 
sde           8:64   0  511M  0 disk  
â””â”€mpathb    253:4    0  511M  0 mpath 
```

## 3. Formatage et montage

ğŸŒ **CrÃ©ez une partition sur les devices `mpatha` et `mpathb`**

- avec `fdisk`

ğŸŒ **Formatez en `xfs` les partitions**

- avec `mkfs`

ğŸŒ **Point de montage `/mnt/data_chunk1`**

- crÃ©ez-le avec `mkdir`
- utilisez une commande `mount` pour y monter `mpatha`
- Ã©crivez un unitÃ© systemd de type `mount` pour avoir un montage automatique
- ajoutez un `After=network-online.target` pour pas qu'il dÃ©marre avant le rÃ©seau et ne bloque pas le boot

> *Impossible Ã  faire proprement avec juste du `/etc/fstab` ce dÃ©marrage aprÃ¨s le rÃ©seau !*

ğŸŒ **Point de montage `/mnt/data_chunk2`**

- pareil, pour `mpathb`

Ca donne Ã§a :

```bash
[it4@chunk1 ~]$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINTS
sda           8:0    0    8G  0 disk  
â”œâ”€sda1        8:1    0    1G  0 part  /boot
â””â”€sda2        8:2    0    7G  0 part  
  â”œâ”€rl-root 253:0    0  6.2G  0 lvm   /
  â””â”€rl-swap 253:1    0  820M  0 lvm   [SWAP]
sdb           8:16   0  511M  0 disk  
â””â”€mpathb    253:4    0  511M  0 mpath 
  â””â”€mpathb1 253:5    0  503M  0 part  
sdc           8:32   0  511M  0 disk  
â””â”€mpatha    253:2    0  511M  0 mpath 
  â””â”€mpatha1 253:3    0  503M  0 part  
sdd           8:48   0  511M  0 disk  
â””â”€mpatha    253:2    0  511M  0 mpath 
  â””â”€mpatha1 253:3    0  503M  0 part  
sde           8:64   0  511M  0 disk  
â””â”€mpathb    253:4    0  511M  0 mpath 
  â””â”€mpathb1 253:5    0  503M  0 part

[it4@chunk1 ~]$ df -h 
Filesystem           Size  Used Avail Use% Mounted on
devtmpfs             4.0M     0  4.0M   0% /dev
tmpfs                229M     0  229M   0% /dev/shm
tmpfs                 92M  2.5M   89M   3% /run
/dev/mapper/rl-root  6.2G  2.5G  3.7G  41% /
/dev/sda1            960M  280M  681M  30% /boot
tmpfs                 46M  4.0K   46M   1% /run/user/1000
/dev/mapper/mpatha1  439M  149M  291M  34% /mnt/data_chunk1
/dev/mapper/mpathb1  439M  206M  234M  47% /mnt/data_chunk2
```

## 4. Tests

On va simuler une coupure rÃ©seau, en coupant l'un des liens sur `chunk1.tp2.b3`.

On va suivre en temps rÃ©el la bascule iSCSI.

L'idÃ©e :

- on lance une Ã©criture continue sur le volume montÃ© en multipath avec un ptit boucle shell
  - `while true; do date >> /mnt/iscsidisk/date; done`
  - cette ligne fait le taff, elle Ã©crit continuellement l'heure dans un fichier
  - comme Ã§a on verra quand Ã§a stoppe ou non les Ã©critures
- on coupe une interface rÃ©seau
  - utilisez une commande `date` juste avant de couper, pour savoir l'heure prÃ©cise Ã  laquelle vous coupez
  - rallumez au bout de ~30 secondes/1 minute (pareil, en tapant `date` avant)
- on observe la bascule iSCSI
  - on peut regarder les logs en temps rÃ©el, et observer aussi la sortie de `multipath -ll` : `watch -n 0.5 multipath -ll`

### A. Simulation de panne

ğŸŒ **Simuler une coupure rÃ©seau**

- couper un lien rÃ©seau
- surveiller les logs en temps rÃ©el
  - je veux quelques lignes de logs qui montre la bascule dans le compte-rendu
- surveiller `multipath -ll`
  - je veux bien un avant/aprÃ¨s dans le compte-rendu

### B. Jouer avec les paramÃ¨tres

Pour amÃ©liorer la rapiditÃ© de la bascule (attention si le rÃ©seau est instable, Ã§a peut coÃ»ter cher), on peut ajouter un peu de conf et dÃ©terminer des temps plus courts pour les timeouts par exemple.

Je vous laisse aller lire le dÃ©tails des options, je vous recommande de tester avec ces paramÃ¨tres dans `/etc/multipath.conf` :

```conf
defaults {
 user_friendly_names yes
 find_multipaths yes
 path_grouping_policy failover
 no_path_retry 100
 features "1 queue_if_no_path"
 polling_interval 2
 max_polling_interval 8
 fast_io_fail_tmo 3
}
```

ğŸŒ **Resimuler une panne**

- on devrait observer une bascule lÃ©gÃ¨rement plus rapide

## 5. Replicate

âœ **RÃ©pliquer le setup de `chunk1` sur les machines `chunk2` et `chunk3`**

- chaque machine ne doit accÃ©der qu'aux targets iSCSI qui lui sont destinÃ©s
  - chaque machine "chunk" a un IQN qui lui est dÃ©diÃ©
  - la machine `chunk1`, on lui a donnÃ© accÃ¨s qu'aux IQN `iqn.2024-12.tp2.b3:chunk1` (sur chaque IP)
  - il faut donc faire pareil la machine `chunk2` et sur `chunk3`
- mÃªme setup :
  - `iscsiadm` pour les 4 targets iSCSI
  - puis multipath pour en avoir plus que 2
  - vous formatez en `xfs` les deux volumes `mpatha` et `mpathb` et montez sur `/mnt/data_chunk1` et `/mnt/data_chunk2`

ğŸŒ **Preuve du setup**

- sur `chunk2.tp2.b3` et sur `chunk3.tp2.b3`
- les commandes suivantes :

```bash
$ lsblk
$ df -h
$ iscsiadm -m session -P 3
$ multipath -ll
```

---

**On a un setup trÃ¨s solide, un ptit SAN modeste, mais solide !**

- **2 serveurs de stockage**
  - configuration RAID
  - qui sont exposÃ©s comme des targets iSCSI
- **3 serveurs qui accÃ¨dent en multipath Ã  2 volumes**
  - tolÃ©rant aux pannes rÃ©seau

**Et le tout est trÃ¨s facile Ã  faire *scale* (Ã§a se script/ansible trÃ¨s bien)** :

- agrandir les RAID
- ajouter des RAID/targets
- ajouter un initiator

Let's go, on enchaÃ®ne sur un setup de systÃ¨me de fichiers distribuÃ© sur ces 3 serveurs "chunk" (donc le nom va prendre sens).
