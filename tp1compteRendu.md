# TP1 : Single-machine storage RÃ©ponse

# 0. PrÃ©requis

âœ **une VM  Debian 12 <span style="color: red;">sans interface graphique</span>**

![problem](https://media.tenor.com/LoNa2zOMxoAAAAAM/its-very-important-it-matters.gif)

# I. Fundamentals

ğŸŒ **Lister tous les pÃ©riphÃ©riques de stockage branchÃ©s Ã  la VM**

- `lsblk -d`

ğŸŒ **Lister toutes les partitions des pÃ©riphÃ©riques de stockage**

- `lsblk -f`

ğŸŒ **Effectuer un test SMART sur le disque**

- Installer l'outil smartmontools
- Testez la santÃ© du disque principal /dev/sda : `smartctl -H /dev/sda`

- Affichez des informations dÃ©taillÃ©es sur le disque : `sudo smartctl -a /dev/sda`

- Malheureusement , les commandes ci-dessus ne fonctionnent pas sur les disques virtuel de virtual box

![problem](https://media1.tenor.com/m/VpALCb0oXrQAAAAd/oss117-alerte-rouge-en-afrique-noire.gif)

ğŸŒ **Espace disque : afficher l'espace disque restant sur la partition `/`**

- `df -h /`

ğŸŒ **Inodes**

- afficher la quantitÃ© d'inodes restants: `df -i /`

ğŸŒ **Latence disque**

- Installer ioping 

- utilisez `ioping` pour dÃ©terminer la latence disque :  `ioping -c 10 -d 1G /dev/sda1`

ğŸŒ **DÃ©terminer la taille du cache *filesystem***

- `free -h`

# II. Partitioning

Ici on utilise un des disques supplÃ©mentaires branchÃ©s Ã  la VM : `sdb`.


ğŸŒ **Ajouter `sdb` comme Physical Volume LVM**

- CrÃ©er un volume physique (PV) sur le disque sdb : `sudo pvcreate /dev/sdb`

ğŸŒ **CrÃ©er un Volume Group LVM nommÃ© `storage`**

- `sudo vgcreate storage /dev/sdb`

ğŸŒ **CrÃ©er un Logical Volume LVM**

- dans le VG `storage`
- nommÃ© `smol_data`
- sa taille : 2G
- La commande est : `sudo lvcreate -L 2G -n smol_data storage`

ğŸŒ **CrÃ©er un deuxiÃ¨me Logical Volume LVM**

- dans le VG `storage`
- nommÃ© `big_data`
- sa taille : tout le reste du VG
- La commande est : `sudo lvcreate -l 100%FREE -n big_data storage`

ğŸŒ **CrÃ©ez un systÃ¨me de fichiers sur les deux LVs**

 

`sudo mkfs.ext4 /dev/storage/smol_data`

   `sudo mkfs.ext4 /dev/storage/big_data`

ğŸŒ **Montez la partition `smol_data`**

- CrÃ©ation du point de montage : `sudo mkdir -p /mnt/lvm_storage/smol`
- Montage des volumes : `sudo mount /dev/storage/smol_data /mnt/lvm_storage/smol`

ğŸŒ **Montez la partition `big_data`**

- CrÃ©ation du point de montage : `sudo mkdir -p /mnt/lvm_storage/big`
- Montage des volumes : `sudo mount /dev/storage/big_data /mnt/lvm_storage/big`

ğŸŒ **Configurer un *automount***

- Il n'Ã©tait pas possible de crÃ©er l'automount avec un Debian 12 avec interface de bureau sans raisons apparentes donc j'ai refait le TP avec une machine en sans interface graphique.

- Ajoutez dans /etc/fstab les lignes suivantes :
- `/dev/mapper/storage-smol_data /mnt/lvm_storage/smol ext4 defaults 0 0`
- `/dev/mapper/storage-big_data /mnt/lvm_storage/big ext4 defaults 0 0`


# III. RAID

## 1. Simple RAID

ğŸŒ **Mettre en place un RAID 5**

- avec TROIS disques parmis les disques supplÃ©mentaires branchÃ©s Ã  la VM : `sdc` `sdd` et `sde`
- en utilisant `mdadm`
- La commande est : `sudo mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdc /dev/sdd /dev/sde`

ğŸŒ **Prouvez que le RAID5 est en place**:

- `mdadm --detail /dev/md0`

ğŸŒ **Rendre la configuration automatique au boot de la machine**

- `sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf`

- `sudo update-initramfs -u` 

ğŸŒ **CrÃ©ez un systÃ¨me de fichiers sur la partition proposÃ© par le RAID**

- Formatez le RAID en ext4 : `sudo mkfs.ext4 /dev/md0`

ğŸŒ **Monter la partition sur `/mnt/raid_storage`**

- CrÃ©ation du point de montage : `sudo mkdir -p /mnt/raid_storage`
- Montage des volumes : `sudo mount /dev/md0 /mnt/raid_storage`
- VÃ©rification que la partition est bien montÃ© : `df -h /mnt/raid_storage`
ğŸŒ **Prouvez que...**

- la partition est bien montÃ©e : `lsblk`
- il y a bien l'espace disponible attendue sur la partition : `df -h /mnt/raid_storage`
- vous pouvez lire et Ã©crire sur la partition : `sudo touch /mnt/raid_storage/testfile`, `ls /mnt/raid_storage` 

> *Alors combien de Go dispo avec un RAID5 sur des disques de 10G ?* `20 Go disponibles`

ğŸŒ **Mini benchmark**

- Teste de la vitesse dâ€™Ã©criture sur /mnt/raid_storage : `dd if=/dev/zero of=/mnt/raid_storage/testfile bs=1G count=1 oflag=direct`
- Comparaison avec la partition LVM montÃ©e sur /mnt/lvm_storage/smol : `dd if=/dev/zero of=/mnt/lvm_storage/smol/testfile bs=1G count=1 oflag=direct`

## 2. Break it

ğŸŒ **Simule une panne**

- Marquez /dev/sdd comme dÃ©faillant : `sudo mdadm /dev/md0 --fail /dev/sdd`
- Retirez le disque dÃ©faillant du RAID : `sudo mdadm /dev/md0 --remove /dev/sdd`

ğŸŒ **Montre l'Ã©tat du RAID dÃ©gradÃ©**

- avec une commande, on doit voir que le RAID est dÃ©gradÃ© : `cat /proc/mdstat`
- On peut afficher les dÃ©tails avec `sudo mdadm --detail /dev/md0`

ğŸŒ **Remonte le disque dur**

- On RÃ©intÃ©gre le disque dans le RAID : `sudo mdadm /dev/md0 --add /dev/sdd`

## 3. Spare disk

On ajoute le disque /dev/sdf au RAID en tant que disque spare :
- `sudo mdadm /dev/md0 --add /dev/sdf` 

ğŸŒ **Ajoutez un disque encore inutilisÃ© au RAID5 comme disque de *spare***

- On vÃ©rifie ensuite que le disque spare est bien ajoutÃ© : `sudo mdadm --detail /dev/md0`

ğŸŒ **Simuler une panne**

- Marquez un disque actif comme dÃ©faillant (par exemple, /dev/sdd) : `sudo mdadm /dev/md0 --fail /dev/sdd`
- VÃ©rifier que le disque spare a remplacÃ© le disque dÃ©faillant : `cat /proc/mdstat`

ğŸŒ **Remonter le disque dÃ©branchÃ©**

- RÃ©intÃ©grer le disque dÃ©faillant dans le RAID : `sudo mdadm /dev/md0 --add /dev/sdd`

## 4. Grow

ğŸŒ **Ajoutez un disque encore inutilisÃ© au RAID5 comme disque de *spare***

- Ajout d'un deuxiÃ¨me disque spare (par exemple, /dev/sdg) : `sudo mdadm /dev/md0 --add /dev/sdg`
- VÃ©rifications que les deux disques spare sont bien ajoutÃ©s : `sudo mdadm --detail /dev/md0`

ğŸŒ **Grow !**

- Convertion d'un disque spare en disque actif : `sudo mdadm --grow /dev/md0 --raid-devices=4`

ğŸŒ **Prouvez que le RAID5 propose dÃ©sormais 4 disques actifs**

- alors combien d'espace sur un RAID5 de 4 disques ? `30 Go disponible`

ğŸŒ **Euuuh wait a sec... `/mnt/raid_storage` ???**

- Agrandissement du systÃ¨me de fichiers pour utiliser lâ€™espace supplÃ©mentaire : `sudo resize2fs /dev/md0`

- VÃ©rification que le systÃ¨me de fichiers a bien Ã©tÃ© agrandi : `df -h /mnt/raid_storage`

# IV. NFS

Enfin, on clÃ´t le TP avec un **partage rÃ©seau simple Ã  setup et relativement efficace : NFS.** Il est assez rÃ©pandu dans le monde opensource pour des charges faibles.

C'est **un modÃ¨le de client/serveur** : on installe un serveur NFS sur une machine, on le configure pour indiquer quel(s) dossier(s) on veut partager sur le rÃ©seau, et d'autres machines peuvent s'y connecter pour accÃ©der Ã  ces dossiers.

ğŸŒ **Installer un serveur NFS**
- Installation du server NFS : `sudo apt-get install nfs-server`

- Ajouter les lignes suivantes dans le fichier de conf `/etc/exports` pour partager les points de montages :
`/mnt/raid_storage *(rw,sync)`, `/mnt/lvm_storage *(rw,sync)`

- Application de la configuration : `sudo exportfs -arv`

    - a : Exporte tous les systÃ¨mes de fichiers.
    - r : RÃ©exporte tous les rÃ©pertoires.
    - v : Affiche les informations des exports.

- VÃ©rification des exports: `sudo exportfs -v`

- Lancement du service NFS : `sudo systemctl start nfs-server`

- Il faut maintenant Configurer le firwall pour le partage NFS ne soit actif que sur mon rÃ©seau (192.168.248.0):

    - `sudo ufw allow from 192.168.248.0/24 to any port 2049 proto tcp`

    - `sudo ufw allow from 192.168.248.0/24 to any port 2049 proto udp`

![problem](https://media1.tenor.com/m/XC8c8MZFXVQAAAAd/we-have-a-problem-olsen.gif)

- j'ai n'ai pas rÃ©ussi Ã  monter les partages NFS sur mon client car il manque aussi le port 111 aussi utilisÃ© par NFS:

   - `sudo ufw allow from 192.168.248.0/24 to any port 111 proto tcp`

   - `sudo ufw allow from 192.168.248.0/24 to any port 111 proto udp`

- VÃ©rification du statut du firewall : `sudo ufw status`

ğŸŒ **Configurer le client NFS**

- installer un client NFS pour se connecter au serveur : `sudo apt install nfs-common`

- CrÃ©er les points de montage pour le partage NFS :
  - `sudo mkdir -p /mnt/raid_storage`

  - `sudo mkdir -p /mnt/lvm_storage`

- Montez le partage NFS pour /mnt/raid_storage et /mnt/lvm_storage (en utilisant l'ip du serveur NFS):

     - `sudo mount 192.168.248.10:/mnt/raid_storage /mnt/raid_storage`

     - `sudo mount 192.168.248.10:/mnt/raid_storage /mnt/lvm_storage`

- VÃ©rifier que les partages sont bien montÃ©s : 
    - `df -h`
    - `ls /mnt/raid_storage`
    - `ls /mnt/lvm_storage`

- Rendre persistant les montages des disques en les lignes ci-dessous dans /etc/fstab:

    - `192.168.248.10:/mnt/raid_storage /mnt/raid_storage nfs defaults 0 0`

    - `192.168.248.10:/mnt/lvm_storage /mnt/lvm_storage nfs defaults 0 0`

- Recharger les montages : `sudo mount -a`

ğŸŒ **Benchmarkz**

- faites un test de vitesse d'Ã©criture sur la partition `mdadm` montÃ©e en NFS : `/mnt/raid_storage`:
    
    - `dd if=/dev/zero of=/mnt/raid_storage/testfile bs=1G count=1 oflag=direct`

- faites un test de vitesse d'Ã©criture sur la partition LVM montÃ©e en NFS : `/mnt/lvm_storage`

    - `dd if=/dev/zero of=/mnt/lvm_storage/testfile bs=1G count=1 oflag=direct`


![bobercurva](https://media.tenor.com/fF4sTbrZvnsAAAAM/bober-kurwa.gif)

